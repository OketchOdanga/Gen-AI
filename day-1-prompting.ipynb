{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:23.612251Z","iopub.execute_input":"2025-05-20T10:04:23.612551Z","iopub.status.idle":"2025-05-20T10:04:23.973994Z","shell.execute_reply.started":"2025-05-20T10:04:23.612529Z","shell.execute_reply":"2025-05-20T10:04:23.973042Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:23.975430Z","iopub.execute_input":"2025-05-20T10:04:23.975911Z","iopub.status.idle":"2025-05-20T10:04:32.941032Z","shell.execute_reply.started":"2025-05-20T10:04:23.975852Z","shell.execute_reply":"2025-05-20T10:04:32.939746Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:32.942334Z","iopub.execute_input":"2025-05-20T10:04:32.942665Z","iopub.status.idle":"2025-05-20T10:04:34.721885Z","shell.execute_reply.started":"2025-05-20T10:04:32.942630Z","shell.execute_reply":"2025-05-20T10:04:34.720978Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from google.api_core import retry\n\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:34.724132Z","iopub.execute_input":"2025-05-20T10:04:34.724577Z","iopub.status.idle":"2025-05-20T10:04:35.145303Z","shell.execute_reply.started":"2025-05-20T10:04:34.724542Z","shell.execute_reply":"2025-05-20T10:04:35.144100Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:35.146276Z","iopub.execute_input":"2025-05-20T10:04:35.146691Z","iopub.status.idle":"2025-05-20T10:04:35.339621Z","shell.execute_reply.started":"2025-05-20T10:04:35.146668Z","shell.execute_reply":"2025-05-20T10:04:35.338533Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=\"Explain AI to me like I'm a kid. use 3 sentences\")\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:35.340582Z","iopub.execute_input":"2025-05-20T10:04:35.340851Z","iopub.status.idle":"2025-05-20T10:04:36.688463Z","shell.execute_reply.started":"2025-05-20T10:04:35.340825Z","shell.execute_reply":"2025-05-20T10:04:36.687668Z"}},"outputs":[{"name":"stdout","text":"Imagine a super-smart robot friend that can learn things, like recognizing your toys or helping you with your homework. That robot uses something called Artificial Intelligence, or AI, which is like giving the robot a brain to think and solve problems.  So, AI helps computers do clever things that usually only people can do.\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:36.689333Z","iopub.execute_input":"2025-05-20T10:04:36.689591Z","iopub.status.idle":"2025-05-20T10:04:36.697085Z","shell.execute_reply.started":"2025-05-20T10:04:36.689562Z","shell.execute_reply":"2025-05-20T10:04:36.696158Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Imagine a super-smart robot friend that can learn things, like recognizing your toys or helping you with your homework. That robot uses something called Artificial Intelligence, or AI, which is like giving the robot a brain to think and solve problems.  So, AI helps computers do clever things that usually only people can do.\n"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"Starting a chat","metadata":{}},{"cell_type":"code","source":"chat = client.chats.create(model='gemini-2.0-flash', history=[])\nresponse = chat.send_message('Hello! My name is Austo.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:36.698000Z","iopub.execute_input":"2025-05-20T10:04:36.698278Z","iopub.status.idle":"2025-05-20T10:04:37.105443Z","shell.execute_reply.started":"2025-05-20T10:04:36.698258Z","shell.execute_reply":"2025-05-20T10:04:37.103999Z"}},"outputs":[{"name":"stdout","text":"Hello Austo! It's nice to meet you. How can I help you today?\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"response = chat.send_message('Can you tell me something interesting about dinosaurs?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:37.107364Z","iopub.execute_input":"2025-05-20T10:04:37.107765Z","iopub.status.idle":"2025-05-20T10:04:38.356766Z","shell.execute_reply.started":"2025-05-20T10:04:37.107735Z","shell.execute_reply":"2025-05-20T10:04:38.355890Z"}},"outputs":[{"name":"stdout","text":"Okay, here's an interesting fact about dinosaurs that often surprises people:\n\n**Birds are actually living dinosaurs!**\n\nThat's right! Scientists have found overwhelming evidence that birds evolved from small, feathered theropod dinosaurs. Theropods were a group of carnivorous dinosaurs that included the infamous *Tyrannosaurus Rex* and *Velociraptor*.\n\nEssentially, after the mass extinction event that wiped out most dinosaurs, some of these feathered theropods survived and evolved into the birds we see today. So, the next time you see a robin hopping around, remember you're looking at a direct descendant of a dinosaur!\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:38.359538Z","iopub.execute_input":"2025-05-20T10:04:38.359826Z","iopub.status.idle":"2025-05-20T10:04:38.366524Z","shell.execute_reply.started":"2025-05-20T10:04:38.359804Z","shell.execute_reply":"2025-05-20T10:04:38.365627Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Okay, here's an interesting fact about dinosaurs that often surprises people:\n\n**Birds are actually living dinosaurs!**\n\nThat's right! Scientists have found overwhelming evidence that birds evolved from small, feathered theropod dinosaurs. Theropods were a group of carnivorous dinosaurs that included the infamous *Tyrannosaurus Rex* and *Velociraptor*.\n\nEssentially, after the mass extinction event that wiped out most dinosaurs, some of these feathered theropods survived and evolved into the birds we see today. So, the next time you see a robin hopping around, remember you're looking at a direct descendant of a dinosaur!\n"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"response = chat.send_message('Do you remember what my name is?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:38.367434Z","iopub.execute_input":"2025-05-20T10:04:38.367762Z","iopub.status.idle":"2025-05-20T10:04:38.686313Z","shell.execute_reply.started":"2025-05-20T10:04:38.367742Z","shell.execute_reply":"2025-05-20T10:04:38.685323Z"}},"outputs":[{"name":"stdout","text":"Yes, your name is Austo.\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"for model in client.models.list():\n  print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:38.687221Z","iopub.execute_input":"2025-05-20T10:04:38.687493Z","iopub.status.idle":"2025-05-20T10:04:38.724313Z","shell.execute_reply.started":"2025-05-20T10:04:38.687467Z","shell.execute_reply":"2025-05-20T10:04:38.723358Z"}},"outputs":[{"name":"stdout","text":"models/embedding-gecko-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/gemini-2.5-pro-exp-03-25\nmodels/gemini-2.5-pro-preview-03-25\nmodels/gemini-2.5-flash-preview-04-17\nmodels/gemini-2.5-flash-preview-04-17-thinking\nmodels/gemini-2.5-pro-preview-05-06\nmodels/gemini-2.0-flash-exp\nmodels/gemini-2.0-flash\nmodels/gemini-2.0-flash-001\nmodels/gemini-2.0-flash-lite-001\nmodels/gemini-2.0-flash-lite\nmodels/gemini-2.0-flash-lite-preview-02-05\nmodels/gemini-2.0-flash-lite-preview\nmodels/gemini-2.0-pro-exp\nmodels/gemini-2.0-pro-exp-02-05\nmodels/gemini-exp-1206\nmodels/gemini-2.0-flash-thinking-exp-01-21\nmodels/gemini-2.0-flash-thinking-exp\nmodels/gemini-2.0-flash-thinking-exp-1219\nmodels/learnlm-2.0-flash-experimental\nmodels/gemma-3-1b-it\nmodels/gemma-3-4b-it\nmodels/gemma-3-12b-it\nmodels/gemma-3-27b-it\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\nmodels/aqa\nmodels/imagen-3.0-generate-002\nmodels/gemini-2.0-flash-live-001\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from pprint import pprint\n\nfor model in client.models.list():\n  if model.name == 'models/gemini-2.0-flash':\n    pprint(model.to_json_dict())\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:38.725446Z","iopub.execute_input":"2025-05-20T10:04:38.725701Z","iopub.status.idle":"2025-05-20T10:04:38.790432Z","shell.execute_reply.started":"2025-05-20T10:04:38.725680Z","shell.execute_reply":"2025-05-20T10:04:38.789383Z"}},"outputs":[{"name":"stdout","text":"{'description': 'Gemini 2.0 Flash',\n 'display_name': 'Gemini 2.0 Flash',\n 'input_token_limit': 1048576,\n 'name': 'models/gemini-2.0-flash',\n 'output_token_limit': 8192,\n 'supported_actions': ['generateContent',\n                       'countTokens',\n                       'createCachedContent',\n                       'batchGenerateContent'],\n 'tuned_model_info': {},\n 'version': '2.0'}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from google.genai import types\n\nshort_config = types.GenerateContentConfig(max_output_tokens=200)\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write a 1000 word essay on the importance of olives in modern society.')\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:38.791358Z","iopub.execute_input":"2025-05-20T10:04:38.791609Z","iopub.status.idle":"2025-05-20T10:04:40.393466Z","shell.execute_reply.started":"2025-05-20T10:04:38.791590Z","shell.execute_reply":"2025-05-20T10:04:40.392565Z"}},"outputs":[{"name":"stdout","text":"## The Humble Olive: A Cornerstone of Modern Society\n\nThe olive, a seemingly unassuming fruit, holds a significance that far transcends its small size. From its historical roots as a symbol of peace and prosperity to its modern-day applications in cuisine, health, and industry, the olive tree (Olea europaea) and its versatile fruit have woven themselves into the fabric of human civilization. Understanding the importance of olives in modern society requires acknowledging their profound impact on global economies, dietary habits, cultural identities, and even technological advancements.\n\nOne of the most prominent contributions of olives lies in their integral role in the global food industry. Olive oil, extracted from the fruit, stands as a cornerstone of Mediterranean cuisine and has gained immense popularity worldwide for its culinary versatility and health benefits. Its rich flavor profile, ranging from delicate and fruity to robust and peppery, makes it a prized ingredient in countless dishes, from simple salads and grilled vegetables to complex sauces and baked goods. The increasing awareness of the health risks associated\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"response = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write a short poem on the importance of olives in modern society.')\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:40.394536Z","iopub.execute_input":"2025-05-20T10:04:40.394925Z","iopub.status.idle":"2025-05-20T10:04:41.561806Z","shell.execute_reply.started":"2025-05-20T10:04:40.394896Z","shell.execute_reply":"2025-05-20T10:04:41.561089Z"}},"outputs":[{"name":"stdout","text":"From ancient groves, a humble gift,\nThe olive hangs, a verdant lift.\nPressed into oil, a golden stream,\nA healthy choice, a waking dream.\n\nOn pizza crust, in tapenade,\nA flavor bright, a subtle shade.\nIn martinis dry, a salty treat,\nThe olive reigns, both bitter and sweet.\n\nSo raise a glass, to this small fruit,\nIts impact vast, beyond dispute.\nA taste of history, on every tongue,\nThe olive's story, forever sung.\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Temperature","metadata":{}},{"cell_type":"code","source":"high_temp_config = types.GenerateContentConfig(temperature=2.0)\n\n\nfor _ in range(5):\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=high_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\n  if response.text:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:41.562657Z","iopub.execute_input":"2025-05-20T10:04:41.562919Z","iopub.status.idle":"2025-05-20T10:04:43.094096Z","shell.execute_reply.started":"2025-05-20T10:04:41.562850Z","shell.execute_reply":"2025-05-20T10:04:43.093041Z"}},"outputs":[{"name":"stdout","text":"Orange\n -------------------------\nAzure\n -------------------------\nMagenta\n -------------------------\nTurquoise\n -------------------------\nMagenta\n -------------------------\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"low_temp_config = types.GenerateContentConfig(temperature=0.0)\n\nfor _ in range(5):\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=low_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\n  if response.text:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:43.094974Z","iopub.execute_input":"2025-05-20T10:04:43.095236Z","iopub.status.idle":"2025-05-20T10:04:44.626792Z","shell.execute_reply.started":"2025-05-20T10:04:43.095214Z","shell.execute_reply":"2025-05-20T10:04:44.625788Z"}},"outputs":[{"name":"stdout","text":"Azure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"Top-P \nTop-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\nTop-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.","metadata":{}},{"cell_type":"code","source":"model_config = types.GenerateContentConfig(\n    # These are the default values for gemini-2.0-flash.\n    temperature=1.0,\n    top_p=0.95,\n)\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=story_prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:04:44.627783Z","iopub.execute_input":"2025-05-20T10:04:44.628176Z","iopub.status.idle":"2025-05-20T10:05:51.790085Z","shell.execute_reply.started":"2025-05-20T10:04:44.628145Z","shell.execute_reply":"2025-05-20T10:05:51.789101Z"}},"outputs":[{"name":"stdout","text":"Clementine, a tabby with emerald eyes and a perpetually questioning twitch to her whiskers, was, by all accounts, a house cat. Her days consisted of sunbeam naps, meticulous grooming rituals, and the strategic manipulation of her human, Emily, for extra tuna. But Clementine harbored a secret longing, a yearning for something more than the predictable comforts of her cozy existence.\n\nOne particularly dreary Tuesday, as rain lashed against the windowpanes, Clementine found herself staring at the back gate. Emily, in a fit of pre-work frenzy, had left it slightly ajar. The outside world, usually glimpsed only through the safety of the window, beckoned with an irresistible allure. Adventure called.\n\nWith a decisive flick of her tail, Clementine slipped through the gap. The scent of damp earth and unfamiliar flowers filled her nostrils. This was it. Freedom.\n\nHer adventure began tentatively. She stalked along the fence line, her paws padding softly on the wet grass, a miniature explorer charting unknown territory. She encountered a grumpy robin, who scolded her for trespassing on his worm-hunting grounds. Clementine, unfazed, simply arched her back, puffed up her tail, and hissed, a maneuver she'd perfected during territorial disputes over the sunbeam on the living room rug. The robin, clearly outmatched in the intimidation department, hopped away in a flurry of ruffled feathers.\n\nThe journey led her down an alleyway, a labyrinth of overflowing bins and lurking shadows. A mangy ginger tomcat with a torn ear emerged, his eyes narrowed with suspicion.\n\n\"You new here?\" he growled, his voice a gravelly rasp.\n\nClementine, remembering Emily's strict instructions about avoiding strange cats, instinctively bristled. But the tomcat's eyes, despite their initial hostility, held a flicker of something else - loneliness.\n\n\"I'm Clementine,\" she announced, holding her head high. \"And I'm on an adventure.\"\n\nThe tomcat, whose name, she learned, was Rusty, let out a short, surprisingly gentle chuckle. \"An adventure, eh? Well, this alley ain't exactly the Himalayas, but I suppose it's something.\"\n\nRusty, it turned out, was a reluctant guardian of the alleyway, a weary veteran of countless cat skirmishes. He knew the best hiding spots, the safest routes, and where Mrs. Gable, the kindly old lady who lived in the brick house, left out scraps of chicken.\n\nTogether, Clementine and Rusty explored the alley. She learned to navigate the maze of bins, to sniff out the most promising scraps, and to listen for the telltale rumble of approaching vehicles. She even managed to charm a stray kitten, a scrawny, black creature with enormous ears, who Rusty affectionately called 'Mouse'.\n\nAs the afternoon wore on, the rain stopped, and the sun peeked through the clouds, painting the puddles with gold. Clementine realized she was starting to miss Emily. The thought of the warm sunbeam on the rug, the gentle purr of the washing machine, and the guaranteed tuna were surprisingly appealing.\n\n\"I think I need to go home,\" she announced to Rusty, a hint of sadness in her voice.\n\nRusty nodded, understanding etched on his weathered face. \"Adventure's good,\" he said, \"but sometimes home is better.\"\n\nHe led her back to the fence line, showing her a secret spot where the wooden slats were loose. \"Remember this,\" he whispered, nudging her with his head. \"For next time.\"\n\nClementine slipped back into the garden, unnoticed by Emily. As she curled up in her usual sunbeam, the scent of the alley clinging to her fur, she knew she was no longer just a house cat. She was an adventurer, a friend, a guardian of a secret she would forever cherish. And as she purred contentedly, dreaming of rusty toms and alleyway adventures, she knew, with absolute certainty, that this was just the beginning.\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"** Prompting**","metadata":{}},{"cell_type":"markdown","source":"Zero-shot","metadata":{}},{"cell_type":"code","source":"model_config = types.GenerateContentConfig(\n    temperature=0.1,\n    top_p=1,\n    max_output_tokens=5,\n)\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=zero_shot_prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:05:51.791329Z","iopub.execute_input":"2025-05-20T10:05:51.792415Z","iopub.status.idle":"2025-05-20T10:05:52.163231Z","shell.execute_reply.started":"2025-05-20T10:05:51.792384Z","shell.execute_reply":"2025-05-20T10:05:52.162120Z"}},"outputs":[{"name":"stdout","text":"POSITIVE\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"Enum mode","metadata":{}},{"cell_type":"code","source":"import enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE = \"positive\"\n    NEUTRAL = \"neutral\"\n    NEGATIVE = \"negative\"\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ),\n    contents=zero_shot_prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:05:52.164596Z","iopub.execute_input":"2025-05-20T10:05:52.164998Z","iopub.status.idle":"2025-05-20T10:05:52.644245Z","shell.execute_reply.started":"2025-05-20T10:05:52.164963Z","shell.execute_reply":"2025-05-20T10:05:52.643400Z"}},"outputs":[{"name":"stdout","text":"positive\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"enum_response = response.parsed\nprint(enum_response)\nprint(type(enum_response))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:05:52.645188Z","iopub.execute_input":"2025-05-20T10:05:52.645456Z","iopub.status.idle":"2025-05-20T10:05:52.651412Z","shell.execute_reply.started":"2025-05-20T10:05:52.645434Z","shell.execute_reply":"2025-05-20T10:05:52.650188Z"}},"outputs":[{"name":"stdout","text":"Sentiment.POSITIVE\n<enum 'Sentiment'>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"One-shot and few-shot","metadata":{}},{"cell_type":"markdown","source":"Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt.","metadata":{}},{"cell_type":"code","source":"few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n```\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ),\n    contents=[few_shot_prompt, customer_order])\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:05:52.653595Z","iopub.execute_input":"2025-05-20T10:05:52.654093Z","iopub.status.idle":"2025-05-20T10:05:53.066016Z","shell.execute_reply.started":"2025-05-20T10:05:52.654051Z","shell.execute_reply":"2025-05-20T10:05:53.064811Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"pineapple\"]\n}\n```\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"To provide control over the schema, and to ensure that you only receive JSON (with no other text or markdown), you can use the Gemini API's JSON mode. This forces the model to constrain decoding, such that token selection is guided by the supplied schema.","metadata":{}},{"cell_type":"code","source":"import typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ),\n    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:05:53.067128Z","iopub.execute_input":"2025-05-20T10:05:53.067379Z","iopub.status.idle":"2025-05-20T10:05:53.665301Z","shell.execute_reply.started":"2025-05-20T10:05:53.067360Z","shell.execute_reply":"2025-05-20T10:05:53.664326Z"}},"outputs":[{"name":"stdout","text":"{\n  \"size\": \"large\",\n  \"ingredients\": [\"apple\", \"chocolate\"],\n  \"type\": \"dessert\"\n}\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer directly.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:05:53.666570Z","iopub.execute_input":"2025-05-20T10:05:53.667154Z","iopub.status.idle":"2025-05-20T10:05:54.045714Z","shell.execute_reply.started":"2025-05-20T10:05:53.667116Z","shell.execute_reply":"2025-05-20T10:05:54.044329Z"}},"outputs":[{"name":"stdout","text":"48\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"#stepbystep thinking\nprompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:05:54.047088Z","iopub.execute_input":"2025-05-20T10:05:54.047359Z","iopub.status.idle":"2025-05-20T10:05:54.999359Z","shell.execute_reply.started":"2025-05-20T10:05:54.047338Z","shell.execute_reply":"2025-05-20T10:05:54.998029Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Here's how to solve this problem step-by-step:\n\n1.  **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n\n2.  **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n\n3.  **Determine the partner's current age:** Since the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n\nTherefore, your partner is now 28 years old."},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"ReAct: Reason and act","metadata":{}},{"cell_type":"code","source":"model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:05:55.000566Z","iopub.execute_input":"2025-05-20T10:05:55.000947Z","iopub.status.idle":"2025-05-20T10:05:55.013175Z","shell.execute_reply.started":"2025-05-20T10:05:55.000915Z","shell.execute_reply":"2025-05-20T10:05:55.009529Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\n# You will perform the Action; so generate up to, but not including, the Observation.\nreact_config = types.GenerateContentConfig(\n    stop_sequences=[\"\\nObservation\"],\n    system_instruction=model_instructions + example1 + example2,\n)\n\n# Create a chat that has the model instructions and examples pre-seeded.\nreact_chat = client.chats.create(\n    model='gemini-2.0-flash',\n    config=react_config,\n)\n\nresp = react_chat.send_message(question)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:05:55.015148Z","iopub.execute_input":"2025-05-20T10:05:55.016184Z","iopub.status.idle":"2025-05-20T10:05:55.648804Z","shell.execute_reply.started":"2025-05-20T10:05:55.016145Z","shell.execute_reply":"2025-05-20T10:05:55.647421Z"}},"outputs":[{"name":"stdout","text":"Thought 1\nI need to search for the transformers NLP paper, then find the authors and their ages, and then find the youngest author.\n\nAction 1\n<search>transformers NLP paper</search>\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:05:55.653888Z","iopub.execute_input":"2025-05-20T10:05:55.654290Z","iopub.status.idle":"2025-05-20T10:05:56.205478Z","shell.execute_reply.started":"2025-05-20T10:05:55.654261Z","shell.execute_reply":"2025-05-20T10:05:56.204084Z"}},"outputs":[{"name":"stdout","text":"Thought 2\nI have the list of authors. I need to find their ages and identify the youngest. Since I don't have their ages, I will search each author individually.\n\nAction 2\n<search>Ashish Vaswani</search>\n\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"Thinking mode","metadata":{}},{"cell_type":"markdown","source":"Using a \"thinking mode\" model can provide you with high-quality responses without needing specialised prompting like the previous approaches. One reason this technique is effective is that you induce the model to generate relevant information (\"brainstorming\", or \"thoughts\") that is then used as part of the context in which the final response is generated.","metadata":{}},{"cell_type":"code","source":"import io\nfrom IPython.display import Markdown, clear_output\n\n\nresponse = client.models.generate_content_stream(\n    model='gemini-2.0-flash-thinking-exp',\n    contents='Who was the youngest author listed on the transformers NLP paper?',\n)\n\nbuf = io.StringIO()\nfor chunk in response:\n    buf.write(chunk.text)\n    # Display the response as it is streamed\n    print(chunk.text, end='')\n\n# And then render the finished response as formatted markdown.\nclear_output()\nMarkdown(buf.getvalue())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:05:56.206621Z","iopub.execute_input":"2025-05-20T10:05:56.207140Z","iopub.status.idle":"2025-05-20T10:06:01.925063Z","shell.execute_reply.started":"2025-05-20T10:05:56.207063Z","shell.execute_reply":"2025-05-20T10:06:01.923758Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"That's a great question, but it's actually **not possible to definitively answer** based on publicly available information, including the paper itself (\"Attention Is All You Need\").\n\nHere's why:\n\n1.  **The paper doesn't list authors' ages or birth dates.** Academic papers focus on the research contribution, not personal details like age.\n2.  **Birth dates are generally not public information.** Unless a researcher becomes very famous and shares personal details in interviews or biographies, their exact birth date is usually private.\n\nWhile some authors might have been Ph.D. students at the time (suggesting they might be younger than more established researchers), without knowing the actual birth dates of all eight authors, it's impossible to say definitively who was the youngest.\n\nThe authors of the paper are:\n*   Ashish Vaswani\n*   Noam Shazeer\n*   Niki Parmar\n*   Jakob Uszkoreit\n*   Llion Jones\n*   Aidan N. Gomez\n*   Lukasz Kaiser\n*   Illia Polosukhin\n\nDetermining the youngest among them would require private information."},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"Code prompting","metadata":{}},{"cell_type":"markdown","source":"Generating code\n\nThe Gemini family of models can be used to generate code, configuration and scripts. Generating code can be helpful when learning to code, learning a new language or for rapidly generating a first draft.","metadata":{}},{"cell_type":"code","source":"# The Gemini models love to talk, so it helps to specify they stick to the code if that\n# is all that you want.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ),\n    contents=code_prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:09:49.241901Z","iopub.execute_input":"2025-05-20T10:09:49.242240Z","iopub.status.idle":"2025-05-20T10:09:49.903007Z","shell.execute_reply.started":"2025-05-20T10:09:49.242216Z","shell.execute_reply":"2025-05-20T10:09:49.901810Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n  \"\"\"\n  Calculates the factorial of a number.\n  \"\"\"\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"Code execution","metadata":{}},{"cell_type":"code","source":"from pprint import pprint\n\nconfig = types.GenerateContentConfig(\n    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n)\n\ncode_exec_prompt = \"\"\"\nGenerate the first 14 odd prime numbers, then calculate their sum.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=config,\n    contents=code_exec_prompt)\n\nfor part in response.candidates[0].content.parts:\n  pprint(part.to_json_dict())\n  print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:11:23.091492Z","iopub.execute_input":"2025-05-20T10:11:23.091803Z","iopub.status.idle":"2025-05-20T10:11:33.675810Z","shell.execute_reply.started":"2025-05-20T10:11:23.091781Z","shell.execute_reply":"2025-05-20T10:11:33.674087Z"}},"outputs":[{"name":"stdout","text":"{'text': 'Okay, I can do that. First, I need to generate the first 14 odd '\n         'prime numbers. Remember that a prime number is a number greater than '\n         '1 that has no positive divisors other than 1 and itself. Also, odd '\n         'numbers are integers not divisible by 2. The first prime number is '\n         \"2, which is even, so we'll start with the next prime number, 3, \"\n         'which is odd.\\n'\n         '\\n'\n         \"Then I'll calculate their sum.\\n\"\n         '\\n'}\n-----\n{'executable_code': {'code': 'primes = []\\n'\n                             'num = 3\\n'\n                             'while len(primes) < 14:\\n'\n                             '    is_prime = True\\n'\n                             '    for i in range(2, int(num**0.5) + 1):\\n'\n                             '        if num % i == 0:\\n'\n                             '            is_prime = False\\n'\n                             '            break\\n'\n                             '    if is_prime:\\n'\n                             '        primes.append(num)\\n'\n                             '    num += 2  # Check only odd numbers\\n'\n                             '\\n'\n                             \"print(f'{primes=}')\\n\"\n                             '\\n'\n                             'sum_of_primes = sum(primes)\\n'\n                             \"print(f'{sum_of_primes=}')\\n\",\n                     'language': 'PYTHON'}}\n-----\n{'code_execution_result': {'outcome': 'OUTCOME_OK',\n                           'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, '\n                                     '31, 37, 41, 43, 47]\\n'\n                                     'sum_of_primes=326\\n'}}\n-----\n{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n         '31, 37, 41, 43, and 47. Their sum is 326.\\n'}\n-----\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"Explaining code","metadata":{}},{"cell_type":"code","source":"file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=explain_prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:13:39.563022Z","iopub.execute_input":"2025-05-20T10:13:39.563360Z","iopub.status.idle":"2025-05-20T10:13:44.120040Z","shell.execute_reply.started":"2025-05-20T10:13:39.563335Z","shell.execute_reply":"2025-05-20T10:13:44.118278Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file is a script designed to enhance your command-line prompt by displaying information about the current Git repository. Specifically, it's meant to be sourced (run) from your `.bashrc` or `.zshrc` file to customize your prompt.\n\nHere's a breakdown:\n\n*   **Purpose:** The script aims to show you the Git branch you're on, the status of your working directory (staged changes, uncommitted changes, etc.), and other useful Git-related information directly in your command prompt.  This makes it easier to stay aware of your Git status without having to run `git status` constantly.\n\n*   **What it does:**\n    *   **Detects Git repository:** It checks if the current directory is part of a Git repository.\n    *   **Fetches Git status:** It uses Git commands to determine the current branch, the presence of staged, unstaged, or untracked files, and the status of the remote repository (ahead, behind, etc.).\n    *   **Formats the prompt:**  It formats this information into a concise string, using colors and symbols to make it easily readable. It uses themes to customize the look and feel.\n    *   **Sets the prompt:** It modifies your shell's `PS1` variable (the primary prompt string) to include the Git information.\n    *   **Asynchronous fetching:** It can optionally fetch remote status updates in the background to avoid slowing down your prompt.\n    *   **Customization:** It allows you to customize the prompt's appearance using themes, symbols, and variables defined in separate files.\n    *   **Virtualenv support:**  It includes support for displaying the active virtual environment (Python, Node, Conda).\n\n*   **Why you'd use it:**\n    *   **Improved workflow:** Quickly see your Git status at a glance, avoiding the need to run `git status` repeatedly.\n    *   **Increased awareness:** Helps you stay on top of your Git workflow and avoid accidental commits or other mistakes.\n    *   **Customization:** Tailor the prompt to your preferences, choosing colors, symbols, and the information you want to see.\n    *   **Visual reminders:** Provides a visual reminder of your current branch and the state of your working directory.\n\nIn summary, it's a convenience tool that improves your Git workflow by integrating Git status information directly into your command-line prompt.  It's highly configurable and can make working with Git repositories much more efficient.\n"},"metadata":{}}],"execution_count":35}]}